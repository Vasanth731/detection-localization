# imports
import torch 
import torch.nn as nn
import numpy as np
import cv2
import os
from torchvision import datasets
import torchvision.transforms as T
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
# from torchvision.models.detection import maskrcnn_resnet50_fpn
import json
import math
from functools import partial
import torchvision.models as models
import torch.nn.functional as F
from skimage.filters import threshold_otsu
from skimage import morphology



# hyperparameters 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_size = 224 * 224 * 3
# pretrain_path = "/home/htic/Videos/resnet_checkpoint/DINO_best_dino_resnet_classifier_checkpoint.pth"
pretrain_path = "/opt/algorithm/DINO_best_dino_resnet_classifier_checkpoint.pth"
# output_dir = "/home/htic/Pictures/frame_clip59"
# output_dir = "/input/frame_clip59/" # ./data
# video_path = "/home/htic/Downloads/clip_000059.mp4"
# video_path = "/input/clip_000117.mp4"

# video_path = "/input/vid_1_short.mp4"

video_path = "/input/endoscopic-robotic-surgery-video.mp4"

# folder = [os.path.join(output_dir, filename) for filename in os.listdir(output_dir) if filename.lower().endswith(('.png', '.jpg', '.jpeg'))]
# frames_to_save = [10]



# writing the classes needed to be used as models 

# utils for ViT

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    #if (mean < a - 2 * std) or (mean > b + 2 * std):
        #warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      #"The distribution of values may be incorrect.",
                      #stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor
def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class MultiCropWrapper(nn.Module):
    def __init__(self, backbone, head):
        super(MultiCropWrapper, self).__init__()
        # disable layers dedicated to ImageNet labels classification
        backbone.fc, backbone.head = nn.Identity(), nn.Identity()
        self.backbone = backbone
        self.head = head

    def forward(self, x):
        # convert to list
        if not isinstance(x, list):
            x = [x]
        idx_crops = torch.cumsum(torch.unique_consecutive(
            torch.tensor([inp.shape[-1] for inp in x]),
            return_counts=True,
        )[1], 0)
        start_idx, output = 0, torch.empty(0).to(x[0].device)
        for end_idx in idx_crops:
            _out = self.backbone(torch.cat(x[start_idx: end_idx]))
            # The output is a tuple with XCiT model. See:
            # https://github.com/facebookresearch/xcit/blob/master/xcit.py#L404-L405
            if isinstance(_out, tuple):
                _out = _out[0]
            # accumulate outputs
            output = torch.cat((output, _out))
            start_idx = end_idx
        # Run the head forward on the concatenated features.
        return self.head(output)




############################### ViT ######################################

def drop_path(x, drop_prob: float = 0., training: bool = False):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x, attn


class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x, return_attention=False):
        y, attn = self.attn(self.norm1(x))
        if return_attention:
            return attn
        x = x + self.drop_path(y)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        num_patches = (img_size // patch_size) * (img_size // patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


class VisionTransformer(nn.Module):
    """ Vision Transformer """
    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,
                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,
                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim

        self.patch_embed = PatchEmbed(
            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks = nn.ModuleList([
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
            for i in range(depth)])
        self.norm = norm_layer(embed_dim)

        # Classifier head
        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        trunc_normal_(self.pos_embed, std=.02)
        trunc_normal_(self.cls_token, std=.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def interpolate_pos_encoding(self, x, w, h):
        npatch = x.shape[1] - 1
        N = self.pos_embed.shape[1] - 1
        if npatch == N and w == h:
            return self.pos_embed
        class_pos_embed = self.pos_embed[:, 0]
        patch_pos_embed = self.pos_embed[:, 1:]
        dim = x.shape[-1]
        w0 = w // self.patch_embed.patch_size
        h0 = h // self.patch_embed.patch_size
        w0, h0 = w0 + 0.1, h0 + 0.1
        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),
            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),
            mode='bicubic',
        )
        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]
        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)

    def prepare_tokens(self, x):
        B, nc, w, h = x.shape
        x = self.patch_embed(x)  # patch linear embedding

        # add the [CLS] token to the embed patch tokens
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)

        # add positional encoding to each token
        x = x + self.interpolate_pos_encoding(x, w, h)

        return self.pos_drop(x)

    def forward(self, x):
        x = self.prepare_tokens(x)
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        return x[:, 0]

    def get_last_selfattention(self, x):
        x = self.prepare_tokens(x)
        for i, blk in enumerate(self.blocks):
            if i < len(self.blocks) - 1:
                x = blk(x)
            else:
                # return attention of the last block
                return blk(x, return_attention=True)

    def get_intermediate_layers(self, x, n=1):
        x = self.prepare_tokens(x)
        # we return the output tokens from the `n` last blocks
        output = []
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if len(self.blocks) - i <= n:
                output.append(self.norm(x))
        return output


def vit_tiny(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_small(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_base(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


class DINOHead(nn.Module):
    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.mlp = nn.Linear(in_dim, bottleneck_dim)
        else:
            layers = [nn.Linear(in_dim, hidden_dim)]
            if use_bn:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.GELU())
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_bn:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(nn.GELU())
            layers.append(nn.Linear(hidden_dim, bottleneck_dim))
            self.mlp = nn.Sequential(*layers)
        self.apply(self._init_weights)
        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))
        self.last_layer.weight_g.data.fill_(1)
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.mlp(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x


#################################### DINO Classifier #################################################

# ResNet 

class BasicBlock(nn.Module):
    def __init__(self, in_channels, intermediate_channels, identity_downsample=None, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(intermediate_channels)
        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(intermediate_channels)
        self.relu = nn.ReLU()
        self.identity_downsample = identity_downsample
        self.stride = stride

    def forward(self, x):
        identity = x.clone()

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)

        if self.identity_downsample is not None:
            identity = self.identity_downsample(identity)

        x += identity
        x = self.relu(x)
        return x


class ResNet(nn.Module):
    def __init__(self, block, layers, image_channels, num_classes):
        super(ResNet, self).__init__()
        self.in_channels = 64
        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=64, stride=1)
        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=128, stride=2)
        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=256, stride=2)
        self.layer4 = self._make_layer(block, layers[3], intermediate_channels=512, stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x

    def _make_layer(self, block, num_blocks, intermediate_channels, stride):
        identity_downsample = None
        layers = []

        if stride != 1 or self.in_channels != intermediate_channels:
            identity_downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, intermediate_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(intermediate_channels)
            )

        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride))
        self.in_channels = intermediate_channels

        for _ in range(1, num_blocks):
            layers.append(block(self.in_channels, intermediate_channels))

        return nn.Sequential(*layers)


def ResNet18(img_channel=3, num_classes=14):
    return ResNet(BasicBlock, [2, 2, 2, 2], img_channel, num_classes)



def freeze(model, freeze: bool):
    for parameter in model.parameters():
        parameter.requires_grad_(not freeze)


class dino_classifier(nn.Module):
    def __init__(self,input):
        super(dino_classifier, self).__init__()
        #hyperparameters 
        DROP_PATH_RATE = 0.1
        PATCH_SIZE = 16
        OUT_DIM = 65536
        new_height, new_width = 224, 224
        # pretrain_path_resnet18 = "/home/htic/Videos/resnet18.pth"
        pretrain_path_resnet18 = "/opt/algorithm/resnet18.pth"
        # pretrain_path = "/home/htic/Videos/checkpoint/DINO_best_student.pth"
        pretrain_path = "/opt/algorithm/DINO_best_student.pth"

        self.student_pretrained_statedict = torch.load(pretrain_path, map_location=torch.device('cpu'))
        self.student = vit_small(drop_path_rate=DROP_PATH_RATE,patch_size=PATCH_SIZE)
        self.student_multicrop = MultiCropWrapper(self.student, DINOHead(
                                                                    in_dim = self.student.embed_dim,
                                                                    out_dim=OUT_DIM,
                                                                    use_bn=False,
                                                                    norm_last_layer=True,
                                                                    ))
        
        self.student_multicrop.load_state_dict(self.student_pretrained_statedict["state_dict"]) 

        self.conv = nn.Conv2d(in_channels=9, out_channels=3, kernel_size=1)

        self.resnet = ResNet18()
        self.pretrained_state_dict = torch.load(pretrain_path_resnet18,  map_location=torch.device('cpu'))
        self.pretrained_model = self.resnet
        self.pretrained_state_dict = self.pretrained_model.state_dict()
        self.model_state_dict = self.resnet.state_dict()

        # Filter out unnecessary keys and load pretrained weights
        self.pretrained_state_dict = {k: v for k, v in self.pretrained_state_dict.items() if k in self.model_state_dict}
        self.model_state_dict.update(self.pretrained_state_dict)
        self.resnet.load_state_dict(self.model_state_dict)

        # freeze(self.student_multicrop,True)    
        # freeze the layers before this and only update the below layer
        # self.resized_attention_map = torch.nn.functional.interpolate(attention_map, size=(new_height, new_width), mode='bilinear', align_corners=False)
        # self.MLP =MLP(OUT_DIM,14)

        
    def forward(self, input_image):
        attention_map = self.student_multicrop.backbone.get_last_selfattention(input_image)
        resized_attention_map = torch.nn.functional.interpolate(attention_map, size=(224, 224), mode='bilinear', align_corners=False)

        concatenated = torch.concat((input_image,resized_attention_map),dim = 1)
        channel_reduction = self.conv(concatenated)
        # unet = UNET(channel_reduction)
        x = self.resnet(channel_reduction)
        # x = self.MLP(unet)
        # x = MLP(input = unet,input_size=1*224*224,num_classes=14)
        return x

############################## Segmentation ####################################


def load_config_file(config_file):

    all_params = json.load(open(config_file))
    print(all_params)

    return all_params


def min_max_normalize(x):
    min_x = x.min()
    max_x = x.max()
    if (max_x - min_x) == 0:
        return x - min_x
    else:
        return (x - min_x) / (max_x - min_x)


def get_anchor_loss(pos_logits, neg_logits,
    pos_gt, neg_gt, pos_ratio, neg_ratio, naming='anc'):

    pos_log_probs = F.logsigmoid(pos_logits)
    neg_log_probs = F.logsigmoid(neg_logits)

    loss_pos = (- pos_gt * pos_log_probs).mean() * pos_ratio 
    loss_neg = (- neg_gt * neg_log_probs).mean() * neg_ratio

    loss = loss_pos + loss_neg

    loss_details = {
        '{}_pos'.format(naming): loss_pos.item(),
        '{}_neg'.format(naming): loss_neg.item(),
        '{}_sum'.format(naming): loss.item(),
    }

    return loss, loss_details


def get_diffusion_loss(feature_map_1, feature_map_2, 
    logits_1, logits_2, fg_margin, bg_margin, 
    fg_ratio, bg_ratio, naming='diff'):

    probs_1 = torch.sigmoid(logits_1).permute(0, 3, 1, 2)
    probs_2 = torch.sigmoid(logits_2).permute(0, 3, 1, 2)

    size = (feature_map_1.shape[2], feature_map_1.shape[3])

    probs_1 = F.interpolate(probs_1, size=size, mode='bicubic', align_corners=True)

    probs_2 = F.interpolate(probs_2, size=size, mode='bicubic', align_corners=True)

    fg_1 = (probs_1 * feature_map_1).sum(2).sum(2) / (probs_1.sum(2).sum(2) + 1e-10)
    fg_2 = (probs_2 * feature_map_2).sum(2).sum(2) / (probs_2.sum(2).sum(2) + 1e-10)
    
    bg_1 = ((1 - probs_1) * feature_map_1).sum(2).sum(2) / ((1 - probs_1).sum(2).sum(2) + 1e-10)
    bg_2 = ((1 - probs_2) * feature_map_2).sum(2).sum(2) / ((1 - probs_2).sum(2).sum(2) + 1e-10)

    sim = nn.CosineSimilarity(dim=1)

    fg_diff = sim(fg_1, fg_2) - sim(fg_1, bg_1) / 2 - sim(fg_2, bg_2) / 2
    bg_diff = sim(bg_1, bg_2) - sim(fg_1, bg_1) / 2 - sim(fg_2, bg_2) / 2

    loss_fg = F.relu(fg_margin - fg_diff).mean() * fg_ratio
    loss_bg = F.relu(bg_margin - bg_diff).mean() * bg_ratio

    loss = loss_fg + loss_bg

    loss_details = {
        '{}_fg'.format(naming): loss_fg.item(),
        '{}_bg'.format(naming): loss_bg.item(),
        '{}_sum'.format(naming): loss.item(),
    }

    return loss, loss_details


def get_threshold(pred):

    try:
        threshold = threshold_otsu(pred)
    except ValueError:  # all same values
        threshold = pred.mean()

    return threshold


def get_mask(pred, threshold=None, percentile=None, lower_bound=None):

    if (threshold is None) and (percentile is None):
        threshold = get_threshold(pred) 

    elif (threshold is None) and (percentile is not None):
        threshold = np.percentile(pred, percentile)

    elif (threshold is not None) and (percentile is None):
        pass
    else:
        raise Exception('Get Mask Error.')

    if lower_bound:
        threshold = max(threshold, lower_bound)

    return pred > threshold


def get_iou(pred, gt):

    pred = pred > 0
    gt = gt > 0

    intersection = (pred * gt).sum()
    union = pred.sum() + gt.sum() - intersection

    return (intersection + 1e-15) / (union + 1e-15)      


def get_dice(pred, gt):

    pred = pred > 0
    gt = gt > 0

    intersection = (pred * gt).sum()
    union = pred.sum() + gt.sum()

    return (2 * intersection + 1e-15) / (union + 1e-15) 


def get_seg_block(scale_factor, in_channels, out_channels, transposed_conv, align_corners):
    
    module_list = []
    
    if scale_factor is not None:
        
        if align_corners:
            module_list.append(
                nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)
            )
        else:
            module_list.append(
                nn.Upsample(scale_factor=scale_factor)
            )
    
    if transposed_conv:
        module_list.append(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, padding=1)
        )
        module_list.append(
            nn.ReLU(inplace=True)
        )
        module_list.append(
            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, padding=1)
        )
        module_list.append(
            nn.ReLU(inplace=True)
        )
    else:
        module_list.append(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        )
        module_list.append(
            nn.ReLU(inplace=True)
        )
        module_list.append(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        )
        module_list.append(
            nn.ReLU(inplace=True)
        )        
    
    return nn.Sequential(*module_list)
        

class MyUNet(nn.Module):

    def __init__(self, transposed_conv=True, align_corners=False):
        super(MyUNet, self).__init__()
        
        self.backbone = list(models.resnet18(pretrained=False).children())
        self.backbone_bottom = nn.Sequential(*self.backbone[:4])
        self.backbone_layer1 = self.backbone[4]
        self.backbone_layer2 = self.backbone[5]
        self.backbone_layer3 = self.backbone[6]
        self.backbone_layer4 = self.backbone[7]

        self.scale_factor = 32  # 4x1x2x2x2

        self.seg_block1 = get_seg_block(2, 512, 64, transposed_conv, align_corners)
        self.seg_block2 = get_seg_block(2, 320, 32, transposed_conv, align_corners)
        self.seg_block3 = get_seg_block(2, 160, 16, transposed_conv, align_corners)
        self.seg_block4 = get_seg_block(None, 80, 16, transposed_conv, align_corners)
        self.seg_block5 = get_seg_block(4, 80, 8, transposed_conv, align_corners)

        self.seg_head = nn.Linear(11, 1)

    def forward(self, x_0):

        assert(x_0.shape[2] % self.scale_factor == 0)
        assert(x_0.shape[3] % self.scale_factor == 0)

        x_1 = self.backbone_bottom(x_0)
        x_2 = self.backbone_layer1(x_1)
        x_3 = self.backbone_layer2(x_2)
        x_4 = self.backbone_layer3(x_3)
        x_5 = self.backbone_layer4(x_4)

        x_4_u = self.seg_block1(x_5) 
        x_4_c = torch.cat((x_4_u, x_4), 1) 

        x_3_u = self.seg_block2(x_4_c)  
        x_3_c = torch.cat((x_3_u, x_3), 1)

        x_2_u = self.seg_block3(x_3_c)  
        x_2_c = torch.cat((x_2_u, x_2), 1) 

        x_1_u = self.seg_block4(x_2_c) 
        x_1_c = torch.cat((x_1_u, x_1), 1)

        x_0_u = self.seg_block5(x_1_c)  
        x_0_c = torch.cat((x_0_u, x_0), 1)

        x_seg = x_0_c.permute(0, 2, 3, 1)
        x_seg = self.seg_head(x_seg)  

        return x_seg


def out_modify(output):
    probs = torch.sigmoid(output).squeeze().cpu().detach().numpy()
    probs = (probs * 255).astype(np.uint8)
    pred = get_mask(probs, threshold=255 * 0.1)
    pred = morphology.remove_small_objects(pred, 10000)
    return pred

class Segmentation(nn.Module):
    def __init__(self):
        super(Segmentation, self).__init__()
        # hyperparameters
        transposed_conv = False
        align_corners = False
        # path = "/home/htic/Videos/segmentation_checkpoint/SEG_best_segmentation_checkpoint.pth"
        path = "/opt/algorithm/SEG_best_segmentation_checkpoint.pth"
        
        # Load pretrained model
        model_pretrained_statedict = torch.load(path,  map_location=torch.device('cpu'))
        self.model = MyUNet(transposed_conv, align_corners).to(device)
        self.model.load_state_dict(model_pretrained_statedict["state_dict"])

    def forward(self,input_image):
        output = self.model(input_image)
        

        return output


# declaring models and its transforms

path = '/opt/algorithm/entire_model_pretrained_true.pth'
model_1 = torch.load(path).to(device).eval()


# model_1 = maskrcnn_resnet50_fpn(pretrained=True).to(device).eval()  # True 


transform_1 = T.Compose([T.ToTensor(),T.Resize((512, 640))])

model_2 = dino_classifier(input_size)   #.to(device).load_state_dict(torch.load(pretrain_path)["state_dict"])
transform_2 = T.Compose([
            T.Resize(224),
            T.ToTensor(),
            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])

model_3 = Segmentation()
transform_3 = T.Compose([
                        T.Resize((512, 640)),  
                        T.ToTensor(),            
                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])



def crops(image):
    with torch.no_grad():
        box_prediction = model_1(transform_1(image).unsqueeze(0).to(device))[0]


    drawn_image = image.copy()
    drawn_image=drawn_image.resize((640, 512), Image.Resampling.LANCZOS)
    draw = ImageDraw.Draw(drawn_image)


    cropped_images = []
    coordinates = []
    for score, label, box, mask in zip(
        box_prediction['scores'],
        box_prediction['labels'],
        box_prediction['boxes'],
        box_prediction['masks']
    ):
        if score > 0.4:  # Set a threshold for filtering box_predictions
            box = [round(i, 2) for i in box.tolist()]
            x, y, w, h = box
            start_point = (int(x), int(y))
            end_point = (int(x + w), int(y + h))
            draw.rectangle([start_point, end_point], outline="green", width=3)
            cropped_roi = drawn_image.crop((x, y, x + w, y + h))
            resized_roi = cropped_roi.resize((224, 224), Image.BILINEAR)
            cropped_images.append(np.array(resized_roi))

            top_left = start_point
            bottom_left = (int(x), int(y + h))
            bottom_right = end_point
            top_right = (int(x + w), int(y))

            coordinate = [top_left, top_right, bottom_right, bottom_left]
            coordinates.append(coordinate)

            # print("Coordinates:", coordinates)

    print('Number of bounding boxes shown :', len(cropped_images))
    # print('crop coordinates : ',start_point)
    return cropped_images, coordinates


# # for specified frames 
# def extract_frames(video_path):
#     cap = cv2.VideoCapture(video_path)
    
#     # Check if the video file is opened successfully
#     if not cap.isOpened():
#         print("Error: Could not open video file.")
#         return
    
#     frames = []
#     frame_count = 1
    
#     # Loop through each frame in the video
#     while True:
#         ret, frame = cap.read()
        
#         if not ret:
#             break
#         frame_count += 1
        
#         if frame_count in frames_to_save:
#             resized_frame = cv2.resize(frame, (1920, 1080))
            
#             # Crop the resized frame to remove black borders
#             h, w, _ = resized_frame.shape
#             aspect_ratio = 1920 / 1080
#             new_width = int(h * aspect_ratio)
#             resized_frame = resized_frame[:, (w - new_width) // 2 : (w + new_width) // 2]
            
#             frames.append(resized_frame)
            
#             print(f"Extracted frame {frame_count}")
        
#         if frame_count >= max(frames_to_save):
#             break
    
#     cap.release()
    
#     return frames


# # all frames 
def extract_frames(video_path):
    cap = cv2.VideoCapture(video_path)
    
    # Check if the video file is opened successfully
    if not cap.isOpened():
        print("Error: Could not open video file.")
        return
    
    frames = []
    frame_count = 1
    
    # Get the total number of frames in the video
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Loop through each frame in the video
    while True:
        ret, frame = cap.read()
        
        if not ret:
            break
        
        resized_frame = cv2.resize(frame, (1920, 1080))
        
        # Crop the resized frame to remove black borders
        h, w, _ = resized_frame.shape
        aspect_ratio = 1920 / 1080
        new_width = int(h * aspect_ratio)
        resized_frame = resized_frame[:, (w - new_width) // 2 : (w + new_width) // 2]
        
        frames.append(resized_frame)
        
        print(f"Extracted frame {frame_count}/{total_frames}")
        
        frame_count += 1
    
    cap.release()
    
    return frames





def classifier(crop):

    model = model_2 # load the pretrained weights 
    model.to(device)

    model_pretrained_statedict = torch.load(pretrain_path,  map_location=torch.device('cpu'))
    model.load_state_dict(model_pretrained_statedict["state_dict"])
    # model = model_2

    crop = Image.fromarray(crop)
    crop = transform_2(crop).unsqueeze(0).to(device)

    model.eval()
    with torch.no_grad():
        output = model(crop)
        _, predicted_class = torch.max(output, 1)

    # train_dataset = datasets.ImageFolder(root=data_dir, transform=None)

    # Map the predicted class index to the class label
    class_labels = ['bipolar_dissector', 'bipolar_forceps', 'cadiere_forceps', 'clip_applier', 'force_bipolar', 'grasping_retractor', 'monopolar_curved_scissor', 'needle_driver', 'permanent_cautery_hook_spatula', 'prograsp_forceps', 'stapler', 'suction_irrigator', 'tip_up_fenestrated_grasper', 'vessel_sealer']
    

    # class_labels = train_dataset.classes
    predicted_label = class_labels[predicted_class.item()]
    
    # print("Predicted class:", predicted_label)
    return predicted_label, output


def seg_img(input_image):
    input_image = transform_3(input_image).unsqueeze(0).to(device)
    pred = model_3.forward(input_image=input_image)
    pred = out_modify(pred)
    return pred 



if __name__ == "__main__":


    results = []
    extracted_frames = extract_frames(video_path)
    
    z_coordinate = 0.5

    image_results = []

    for i, frame in enumerate(extracted_frames):
        image = Image.fromarray(frame)

        crop_fn, coordinates = crops(image)

        

        frame_result = {
            "type": "Multiple 2D bounding boxes", "boxes": image_results, "version": {"major": 1, "minor": 0}
        }

        for j, img in enumerate(crop_fn):
 
            pred_class, output = classifier(img)
            softmax_output = F.softmax(output, dim=1)
            max_probability, _ = torch.max(softmax_output, dim=1)
            max_probability_rounded = round(max_probability.item(), 3)

            

            for k, box in enumerate(coordinates):
                corners = []
                for corner in box:
                    x, y = corner
                    corners.append([round(x, 1), round(y, 1), 0.5])  # Replace 0.5 with your desired z_coordinate
            
                # print(f'corners: {corners}')

            # for i in coordinates:
            #     print(coordinates)



            # image_result dict loop 
            image_result = {
                "corners": corners, "name": f'slice_nr_{i+1}_{pred_class}', "probability": max_probability_rounded
            }

            # Append the image result to the list of image results for this frame
            image_results.append(image_result)

        
        
        

    # Save the results to a JSON file
    with open('/output/surgical-tools.json', "w") as results_file:
        json.dump(frame_result, results_file)








